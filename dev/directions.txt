Inference scope:
 - Add "infinite" autoregressive inference via output_to_folder (configurable number of rows per file)
 - implement kv_cache (adapt specifically for learned positional embeddings)
Add encoder module with more data types and architectures, for e.g. images
Add "folding"/patching functionality to preprocessing
Add different configurable decoder modules, e.g. mlp (current), transformer, diffusion
Hyperparameter optimization:
 - Add hyperparameter optimization instead of just hyperparameter sampling: bandits/gaussian processes
 - "Dynamic" batch size to maximize GPU utilization
Assert with scheduler_step_on=="batch", on loading the data, that scheduler total steps are larger than epochs*n_batches
Implement sampled softmax
Enable "static" input variables to condition the sequence
Other LLM roadmap:
 - enable tokenization in preprocess and infer with pretrained tokenizer
 - enable temperature/min_p sampling
 - implement tied encoder and decoder weights
 - enable loss masking
 - enable validation set scoring within epochs
Lower/mixed precision training and inference
