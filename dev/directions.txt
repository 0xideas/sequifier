Inference scope:
 - Add "infinite" autoregressive inference via output_to_folder (configurable number of rows per file)
 - implement kv_cache (adapt specifically for learned positional embeddings)
Add encoder module with more data types and architectures, for e.g. images
Add "folding"/patching functionality to preprocessing
Add attention sink token option at beginning of sequence
Add different configurable decoder modules, e.g. mlp (current), transformer, diffusion
Hyperparameter optimization:
 - Add hyperparameter optimization instead of just hyperparameter sampling: bandits/gaussian processes
 - "Dynamic" batch size to maximize GPU utilization
Assert with scheduler_step_on=="batch", on loading the data, that scheduler total steps are larger than epochs*n_batches
Implement sampled softmax
Enable "static" input variables to condition the sequence
Other LLM roadmap:
 - enable tokenization in preprocess and infer with pretrained tokenizer
 - enable temperature/min_p sampling
 - implement tied encoder and decoder weights
 - enable loss masking
 - enable validation set scoring within epochs
Lower/mixed precision training and inference
Add top_k and top_p sampling
Add autoregressive inference until specific values are reached for specific variables
Add total weights and weight distribution tool
Option to leave "target" empty in preprocessing, no offset necessary (for inference on test sets)
num_workers for GPU satutaration in inference
within epoch checkpointing and checkpoint loading
inference resumption
tokenize text into a configurable number of "output" variables during preprocessing
use configurable (external or also sequifier) embedding model to embed text into fixed number of dimensions during preprocessing
"deembedding model export"
if sequifier embedding model used during preprocessing, support deembedding model during inference for generative inference
trajectory transformer: rewrite sequifier/infer.py to optionally implement beam search and integrate a reward-maximization objective.
