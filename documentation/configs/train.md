# Train Command Guide

The `sequifier train` command initializes and trains a causal transformer model based on the sequence data generated during the preprocessing step. It supports custom architectures (e.g., varying layers, heads, embedding sizes), various optimizers (including AdEMAMix), and distributed training strategies.

## Usage

```console
sequifier train --config-path configs/train.yaml
```

## Configuration Fields

The configuration is defined in a YAML file (e.g., `train.yaml`). The file is structured into root-level fields (mostly data/paths) and two subsections: `model_spec` (architecture) and `training_spec` (hyperparameters).

### 1\. File System & Inputs

| Field | Type | Mandatory | Default | Description |
| :--- | :--- | :--- | :--- | :--- |
| `project_root` | `str` | **Yes** | - | The root directory of your Sequifier project. Usually `.` |
| `metadata_config_path`| `str` | **Yes** | - | Path to the JSON file generated by `preprocess`. E.g., `configs/metadata_configs/data.json`. |
| `model_name` | `str` | **Yes** | - | A unique identifier for this training run. Used for naming logs and output files. |
| `training_data_path` | `str` | No | `data/*split0*`| Path to training data. Defaults to split 0 from metadata. |
| `validation_data_path`| `str` | No | `data/*split1*`| Path to validation data. Defaults to split 1 from metadata. |
| `read_format` | `str` | No | `parquet` | Format of input data (`parquet`, `csv`, `pt`). Must match `preprocess` output. |

### 2\. Schema & Columns

| Field | Type | Mandatory | Default | Description |
| :--- | :--- | :--- | :--- | :--- |
| `target_columns` | `list[str]`| **Yes** | - | The specific column(s) the model should learn to predict. |
| `target_column_types`| `dict` | **Yes** | - | Map of target columns to their type: `'categorical'` or `'real'`. |
| `input_columns` | `list[str]`| No | All | Subset of columns to use as input features. Defaults to all available in metadata. |
| `seq_length` | `int` | **Yes** | - | Must match the `seq_length` used in preprocessing. |

### 3\. Model Architecture (`model_spec`)

These fields determine the size and complexity of the Transformer.

| Field | Type | Mandatory | Default | Description |
| :--- | :--- | :--- | :--- | :--- |
| `dim_model` | `int` | **Yes** | - | The internal dimension ($d_{model}$) of the Transformer. |
| `n_head` | `int` | **Yes** | - | Number of attention heads. `dim_model` must be divisible by `n_head`. |
| `num_layers` | `int` | **Yes** | - | Number of transformer encoder layers. |
| `dim_feedforward` | `int` | **Yes** | - | Dimension of the feedforward network model ($d_{ff}$). |
| `initial_embedding_dim`| `int` | **Yes** | - | Size of initial feature embeddings. Usually equals `dim_model`. |
| `joint_embedding_dim` | `int` | No | `null` | If set, projects concatenated inputs to this dim before the transformer. If set, must equal `dim_model`. |
| `feature_embedding_dims`| `dict` | No | `null` | Manual map of column names to embedding sizes. If `null`, sizes are auto-calculated. This works only if there are *only* real or *only* categorical variables, and `initial_embedding_dim` is divisible by the number of variables |
| `activation_fn` | `str` | No | `swiglu` | Activation function: `swiglu`, `gelu`, or `relu`. |
| `attention_type` | `str` | No | `mha` | `mha` (Multi-Head), `mqa` (Multi-Query), or `gqa` (Grouped-Query). |
| `n_kv_heads` | `int` | No | `null` | Number of Key/Value heads for GQA/MQA. If `null`, defaults to `n_head` (standard MHA). |
| `positional_encoding` | `str` | No | `learned`| `learned` (Standard absolute) or `rope` (Rotary Positional Embedding). |
| `rope_theta` | `float` | No | `10000.0` | The base frequency for RoPE. Increase for long-context extrapolation. |
| `normalization` | `str` | No | `rmsnorm`| `rmsnorm` or `layer_norm`. |
| `norm_first` | `bool` | No | `true` | If `true` (Pre-LN), applies normalization before attention/FFN. More stable. |

### 4\. Training Hyperparameters (`training_spec`)

| Field | Type | Mandatory | Default | Description |
| :--- | :--- | :--- | :--- | :--- |
| `device` | `str` | **Yes** | - | `cuda`, `cpu`, or `mps`. |
| `epochs` | `int` | **Yes** | - | Maximum number of training epochs. |
| `batch_size` | `int` | **Yes** | - | Samples per batch. |
| `learning_rate` | `float` | **Yes** | - | Initial learning rate. |
| `dropout` | `float` | No | `0.0` | Dropout probability. |
| `optimizer` | `dict` | No | `{'name': 'Adam'}`| Optimizer config. Supports `Adam`, `AdamW`, `AdEMAMix`, etc. |
| `scheduler` | `dict` | No | `StepLR...` | LR Scheduler config (e.g., `CosineAnnealingLR`). |
| `scheduler_step_on` | `str` | No | `epoch` | When to step the scheduler: `epoch` or `batch`. |
| `criterion` | `dict` | **Yes** | - | Map of target columns to loss functions (e.g., `CrossEntropyLoss`, `MSELoss`). |
| `loss_weights` | `dict` | No | `null` | Weights for combining losses if predicting multiple targets. |
| `class_weights` | `dict` | No | `null` | Weights for specific classes (useful for imbalanced datasets). |
| `save_interval_epochs` | `int` | **Yes** | - | Save a checkpoint every N epochs. |
| `early_stopping_epochs`| `int` | No | `null` | Stop training if validation loss doesn't improve for N epochs. |
| `log_interval` | `int` | No | `10` | Print training logs every N batches. |
| `class_share_log_columns`| `list[str]`| No | `[]` | Columns for which to log the predicted class distribution in validation. |
| `enforce_determinism` | `bool` | No | `false` | Force deterministic algorithms (slower, but reproducible). |
| `num_workers` | `int` | No | `0` | Number of subprocesses for data loading. |
| `max_ram_gb` | `float` | No | `16` | RAM limit (GB) for the cache when using lazy loading. |
| `backend` | `str` | No | `nccl` | The distributed training backend to use (e.g., `nccl` for GPUs, `gloo` for CPUs). Only relevant if `distributed: true`. |
| `device_max_concat_length`| `int` | No | `12` | Controls recursive tensor concatenation to prevent CUDA kernel limits on specific hardware. Lower this if you encounter "CUDA error: too many resources requested for launch". |
| `continue_training` | `bool` | No | `True` | Load model weights and optimizer state from laste checkpoint and continue training |

### 5\. System & Export

| Field | Type | Mandatory | Default | Description |
| :--- | :--- | :--- | :--- | :--- |
| `export_generative_model`| `bool` | **Yes** | - | Export the standard model for next-token prediction. |
| `export_embedding_model` | `bool` | **Yes** | - | Export a model that outputs the vector embedding of the sequence. |
| `inference_batch_size` | `int` | **Yes** | - | Batch size hardcoded into the exported ONNX model. |
| `export_onnx` | `bool` | No | `true` | Export model as `.onnx` for high-performance inference. |
| `export_pt` | `bool` | No | `false`| Export model as `.pt` (PyTorch state dict). |
| `export_with_dropout` | `bool` | No | `false`| Export model with dropout enabled (useful for Monte Carlo Dropout inference). |
| `distributed` | `bool` | No | `false`| Enable multi-GPU training (DDP). Requires `read_format: pt`. |
| `load_full_data_to_ram`| `bool` | No | `true` | If `false`, uses lazy loading (requires `read_format: pt`). |
| `layer_type_dtypes` | `dict` | No | `null` | Map of layer types (`linear`, `embedding`, `norm`, `decoder`) to dtypes (`float32`, `float16`, `bfloat16`, `float8_e4m3fn`, `float8_e5m2`). Used for mixed-precision/quantization. |
| `layer_autocast` | `bool` | No | `true` | If `true`, enables `torch.autocast` for automatic mixed precision training. |

-----

## Key Trade-offs and Decisions

### 1\. Data Loading Strategy (`load_full_data_to_ram`)

  * **`true` (Default):** Loads the entire dataset into system RAM at the start.
      * *Pros:* Fastest training speed (no I/O overhead during epochs).
      * *Cons:* Limited by physical RAM. If the dataset is 64GB and you have 32GB RAM, this will crash.
  * **`false` (Lazy Loading):** Loads individual files on-demand during training.
      * *Requirements:* `read_format` must be `pt`.
      * *Pros:* Can train on datasets significantly larger than RAM. Uses an LRU cache (limit set by `max_ram_gb`) to optimize repeated access.
      * *Cons:* Slight I/O overhead depending on disk speed. Increase `num_workers` to mitigate this.

### 2\. Attention Mechanism (`attention_type` & `n_kv_heads`)

  * **`mha` (Multi-Head Attention - Default):** Standard Transformer attention. Best for general accuracy but memory intensive for the KV cache during inference.
  * **`mqa` (Multi-Query Attention):** Shares a single Key/Value head across all Query heads (`n_kv_heads: 1`). Significantly reduces memory usage during inference and speeds up generation.
  * **`gqa` (Grouped-Query Attention):** A middle ground. Set `n_kv_heads` to a value that divides `n_head` (e.g., 8 heads, 2 KV heads).

### 3\. Activation Function (`activation_fn`)

  * **`swiglu` (Default):** Generally offers better convergence and performance than ReLU or GeLU in modern LLMs (e.g., Llama 2/3).
  * **`gelu` / `relu`:** Standard older activations. Use these if you need strictly smaller models or compatibility with older inference runtimes.

### 4\. Distributed Training (`distributed`)

If you have multiple GPUs:

1.  Set `distributed: true` in `training_spec`.
2.  **Crucial:** You must have run `preprocess` with `write_format: pt` and `merge_output: false`.
3.  Set `world_size` to the number of GPUs.
4.  Sequifier uses `DistributedDataParallel` (DDP) to synchronize gradients across GPUs.

### 5\. Export Formats (`export_generative_model` vs `export_embedding_model`)

  * **Generative:** Exports the full model head. Use this if you want to predict the next token/value (forecasting, generation).
  * **Embedding:** Exports a model that outputs the vector representation of the final token *before* the decoding layer. Use this for clustering, similarity search, or feeding dense features into downstream models (e.g., XGBoost).

-----

## Outputs

After running `train`, the following are generated:

1.  **Models:** Located in `models/`.
      * `sequifier-[NAME]-best-[EPOCH].onnx`: The model with the lowest validation loss.
      * `sequifier-[NAME]-last-[EPOCH].onnx`: The model state at the final epoch.
      * *Note:* If `export_embedding_model: true`, you will also see files suffixed with `-embedding.onnx`.
2.  **Checkpoints:** Located in `checkpoints/`.
      * `.pt` files containing optimizer states, allowing you to resume training later by setting `continue_training: true`.
3.  **Logs:** Located in `logs/`.
      * Detailed logs of training loss, validation loss, and learning rate per epoch/batch.
