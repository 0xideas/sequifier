# Hyperparameter Search Command Guide

The `sequifier hyperparameter-search` command automates the process of finding the optimal model architecture and training configuration. It supports both **Grid Search** (exhaustive) and **Random Sampling** strategies. It creates multiple unique training configurations, executes them sequentially, and logs the results.

## Usage

```console
sequifier hyperparameter-search --config-path configs/hyperparameter_search.yaml
```

## Configuration Fields

The configuration is defined in a YAML file. Unlike the `train.yaml` where fields take single values, most fields here take **lists** of values to search over.

### 1\. File System & Strategy

| Field | Type | Mandatory | Default | Description |
| :--- | :--- | :--- | :--- | :--- |
| `project_root` | `str` | **Yes** | - | The root directory of your Sequifier project. |
| `metadata_config_path`| `str` | **Yes** | - | Path to the JSON metadata file generated by `preprocess`. |
| `hp_search_name` | `str` | **Yes** | - | A prefix for the generated runs (e.g., `my-search`). |
| `model_config_write_path`| `str` | **Yes** | - | Directory to save the generated config files for each run (e.g., `configs/hp_search/`). |
| `search_strategy` | `str` | No | `sample` | `sample` (Random Search) or `grid` (Grid Search). |
| `n_samples` | `int` | *Conditional* | - | Required if `search_strategy` is `sample`. Number of distinct runs to execute. |
| `training_data_path` | `str` | No | Metadata split 0 | Path to training data. |
| `validation_data_path`| `str` | No | Metadata split 1 | Path to validation data. |

### 2\. Schema & Feature Selection

Sequifier allows you to search not just for model parameters, but for the best **subset of input features**.

| Field | Type | Mandatory | Description |
| :--- | :--- | :--- | :--- |
| `input_columns` | `list[list[str]]`| **Yes** | A list of input sets. E.g., `[['col1'], ['col1', 'col2']]`. |
| `target_columns` | `list[str]` | **Yes** | The target column(s) to predict. Fixed across all runs. |
| `seq_length` | `list[int]` | **Yes** | List of sequence lengths to test (e.g., `[24, 48]`). |
| `target_column_types`| `dict` | **Yes** | Map of target columns to `categorical` or `real`. |
| `column_types` | `list[dict]` | *Conditional*| Required if `input_columns` varies. List of type maps corresponding to the input sets. |

### 3\. Model Architecture Sampling (`model_hyperparameter_sampling`)

These fields define the search space for the Transformer architecture. All fields accept a **list** of values.

| Field | Type | Mandatory | Description |
| :--- | :--- | :--- | :--- |
| `dim_model` | `list[int]` | **Yes** | Internal dimension of the Transformer. |
| `num_layers` | `list[int]` | **Yes** | Number of layers. |
| `n_head` | `list[int]` | **Yes** | Number of attention heads. |
| `dim_feedforward` | `list[int]` | **Yes** | Feedforward network dimension. |
| `initial_embedding_dim`| `list[int]` | **Yes** | Feature embedding size. Usually matches `dim_model`. |
| `activation_fn` | `list[str]` | **Yes** | `['swiglu', 'gelu', 'relu']`. |
| `attention_type` | `list[str]` | **Yes** | `['mha', 'mqa', 'gqa']`. |
| `n_kv_heads` | `list[int]` | **Yes** | Number of KV heads (for MQA/GQA). Use `null` for MHA. |
| `normalization` | `list[str]` | **Yes** | `['rmsnorm', 'layer_norm']`. |
| `positional_encoding` | `list[str]` | **Yes** | `['learned', 'rope']`. |

### 4\. Training Hyperparameters (`training_hyperparameter_sampling`)

| Field | Type | Mandatory | Description |
| :--- | :--- | :--- | :--- |
| `learning_rate` | `list[float]`| **Yes** | List of learning rates to test. |
| `batch_size` | `list[int]` | **Yes** | List of batch sizes. |
| `epochs` | `list[int]` | **Yes** | Epochs to train. Paired with `learning_rate`. |
| `dropout` | `list[float]`| No | List of dropout probabilities (default `[0.0]`). |
| `optimizer` | `list[dict]` | No | List of optimizer configs (e.g., `[{'name': 'AdamW'}, {'name': 'AdEMAMix'}]`). |
| `scheduler` | `list[dict]` | No | List of scheduler configs. |
| `accumulation_steps` | `list[int]` | **Yes** | Gradient accumulation steps. |

-----

## Parameter Linkage vs. Independence

To prevent mathematical incompatibilities (e.g., dimension mismatches) and illogical training schedules, the hyperparameter search does **not** perform a simple Cartesian product of every field. Instead, specific parameters are **linked by index**, while others remain **independent**.

### 1\. Linked Parameters (Coupled by List Index)

If you provide a list of $N$ values for an anchor parameter, you **must** provide a list of $N$ values for its linked parameters. The search will strictly pair index $i$ of the anchor with index $i$ of the linked field.

| Group | Anchor Field | Linked Fields (Must match index) | Reason for Linkage |
| :--- | :--- | :--- | :--- |
| **Model Backbone** | `dim_model` | `n_head`<br>`initial_embedding_dim`<br>`joint_embedding_dim`<br>`feature_embedding_dims` | $d_{model}$ determines embedding sizes and must be divisible by the number of heads. |
| **Training Schedule** | `learning_rate` | `epochs`<br>`scheduler` | The magnitude of the learning rate often dictates how many epochs are needed. Schedulers often require `T_max` to match `epochs`. |
| **Data Schema** | `input_columns` | `column_types` | Different subsets of columns require specific data type definitions. |

> **Example:**
> If `dim_model: [64, 128]` and `n_head: [4, 8]`:
>
>   * **Run A** uses `dim_model=64` AND `n_head=4`.
>   * **Run B** uses `dim_model=128` AND `n_head=8`.
>   * *It will NOT attempt `dim_model=64` with `n_head=8`.*

### 2\. Independent Parameters (Cartesian Product)

All other parameters are considered **Independent**. Sequifier will test every value in these lists against every combination of the linked groups above.

  * **Model:** `num_layers`, `dim_feedforward`, `activation_fn`, `normalization`, `positional_encoding`, `attention_type`, `rope_theta`.
  * **Training:** `batch_size`, `dropout`, `accumulation_steps`, `optimizer`.
  * **Data:** `seq_length`.

### 3\. Special Case: `n_kv_heads`

`n_kv_heads` is sampled independently but validated dynamically. If a selected `n_kv_heads` value does not mathematically divide the selected `n_head`, the system automatically reverts to `n_kv_heads = null` (standard Multi-Head Attention) for that run to prevent a crash.

-----

## Key Trade-offs and Decisions

### 1\. `search_strategy`: `grid` vs. `sample`

  * **`grid` (Grid Search):**
      * *How it works:* Generates every possible combination of all provided lists.
      * *Pros:* Exhaustive. You are guaranteed to find the best combination within your search space.
      * *Cons:* Exponential explosion. If you have 4 independent parameters with 5 options each, that's $5^4 = 625$ runs.
  * **`sample` (Random Search):**
      * *How it works:* Randomly selects `n_samples` combinations from the defined space.
      * *Pros:* Much faster. Research suggests random search is often as effective as grid search because neural networks are often sensitive to only a few hyperparameters, which random search explores more efficiently.
      * *Cons:* Results are not deterministic (unless seeded) and might miss the optimal combination.

### 2\. Feature Selection (`input_columns`)

Sequifier uniquely allows you to treat "data" as a hyperparameter.

  * **Usage:** Provide a list of lists.
      * Run 1 might use `['sales', 'day_of_week']`
      * Run 2 might use `['sales', 'day_of_week', 'promotion_flag']`
  * **Benefit:** Helps identify if adding extra features (which increases model size and training time) actually yields better performance or simply adds noise.

### 3\. Automatic Retry Logic

The hyperparameter search command includes **automatic error handling for Out of Memory (OOM)** errors.

  * If a specific run crashes (e.g., CUDA OOM), Sequifier will automatically halve the `batch_size` and retry that specific configuration up to 3 times.
  * *Implication:* You can be aggressive with your `batch_size` in the config; the system will self-correct if it hits hardware limits.

-----

## Outputs

1.  **Generated Configs:** Located in `model_config_write_path` (e.g., `configs/hp_search/`).
      * Files named `[hp_search_name]-run-0.yaml`, `[hp_search_name]-run-1.yaml`, etc.
      * These are valid, standalone `train.yaml` files that you can inspect or re-run manually.
2.  **Logs:** Located in `logs/`.
      * Separate logs for each run, detailing the loss curves.
3.  **Models & Checkpoints:**
      * Saved in `models/` and `checkpoints/` with filenames including the run number (e.g., `models/sequifier-my-search-run-5-best-10.onnx`).
